working_dir: "experiments/model_learning"
learn_mdp: True
skill_learn: False # used by main.py
seed: 7
sweep_name: fpp_mlp
dont_save: False
resume: False
train: True
evaluate: False

world_model:
  validate_only: false
  path_data: ''

  train_params: # parameters for main fct in train.py
    total_grad_steps: 100000
    log_interval: 1000 # global steps
    split_ratios: [0.95, 0.05]
    batch_size: 1000
    lr: 0.001
    gradient_clip: 0.
    init_grad_clip: 0.001
    init_grad_clip_step: 100     # clip gradients in initial N steps to avoid NaNs
    optimizer: "adam"
    adam_beta: 0.9
    val_interval: 1000 # global steps
    normalize_data: False
    loss_fn: 
      name: "nll_loss"

  data_params:
    expert_percent: 1.
    random_percent: 1.
    shrink_dataset: 1.


  model_params:
    model_type: "FactorizedMLP"
    use_state_diff: False
    outp_layer: "GaussianLikelihoodHead"
    outp_layer_params:
        min_var: 1.e-8
        max_var: 200
        use_spectral_norm_var: False #apply spectral normalization to the variance output layer (not to the mean output layer)

    
    hidden_dims: [256, 256, 256]
    weight_init: "orthogonal_"
    bias_init: "zeros_"
    use_spectral_norm: False
    bn_first: True

  env_config:
    name: DisentangledFpp_4Blocks-v1
